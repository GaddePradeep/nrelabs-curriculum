---
- name: Setup DB
  hosts: db_hosts
  become: yes
  tasks:
    - name: Install postgres
      package:
        name: "{{ item }}"
        state: latest
      with_items:
        - postgresql-server
        - postgresql-contrib

    - name: Init DB cluster
      shell: postgresql-setup initdb
      ignore_errors: yes

    - name: Install pg_hba config
      template:
        src: templates/pg_hba.conf.j2
        dest:  /var/lib/pgsql/data/pg_hba.conf
        owner: postgres
        group: postgres
        mode: 0600

    - name: Enable and restart postgresql
      service:
        name: postgresql
        state: restarted
        enabled: yes

- name: Prep instances (sudo)
  hosts: kubernetes_hosts
  become: yes
  tasks:

    - name: Configure kubernetes repo
      template:
        src: templates/kubernetes.repo
        dest:  /etc/yum.repos.d/kubernetes.repo

    - name: Install deps
      package:
        name: "{{ item }}"
        state: latest
      with_items:
        - epel-release
        - python-pip
        - git
        - lsof
        - htop
        - bridge-utils
        - docker
        - kubelet
        - kubeadm
        - kubectl
        - go

    - name: Enable and restart services
      systemd:
        name: "{{ item }}"
        state: restarted
        daemon_reload: yes
        enabled: True
      with_items:
        - kubelet
        - docker

    - name: Enable and restart services
      systemd:
        name: firewalld
        state: stopped
        enabled: False
      with_items:
        - kubelet
        - docker

    - name: Configure kubernetes repo
      template:
        src: templates/sysctl-k8s.conf
        dest:  /etc/sysctl.d/k8s.conf

    - name: Finishing up
      shell: "{{ item }}"
      with_items:
        - setenforce 0
        - sysctl --system
        # - usermod -aG docker $USER
        - iptables -F

    # TODO NOT currently persistent across reboots. Need to do this.
    # echo "loop" | sudo tee /etc/modules-load.d/loop.conf   -----maybe?
    - name: Set up hugepages
      shell: sysctl vm.nr_hugepages=1024

    # TODO Doesn't seem to be working. Still have to run this myself. Also should make this persistent too.
    - name: Load loop kernel module
      shell: modprobe loop

    - name: Download weave CLI utility
      shell: curl -L git.io/weave -o /usr/bin/weave && chmod a+x /usr/bin/weave

# https://kubernetes.io/docs/tasks/tools/install-kubeadm/
# https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
- name: Set up K8S Master
  hosts: kubernetes_master
  become: yes
  tasks:

    - name: Initialize k8s master
      shell: kubeadm init
      register: output_init
      ignore_errors: yes

    # TODO(mierdin): obviously this only works for me right now
    - name: Copy k8s config
      shell: mkdir -p /home/mierdin/.kube && sudo cp -u /etc/kubernetes/admin.conf /home/mierdin/.kube/config && sudo chown -R mierdin:mierdin /home/mierdin/.kube

    - name: debug output_init
      debug:
        msg: "{{ output_init }}"

    - name: pause to let services finish starting
      pause:
        seconds: 30
      
    - name: get master token
      shell: kubeadm token list |grep "The default bootstrap token generated by 'kubeadm init'." |awk '{print $1}'
      register: output

    - name: set master token
      set_fact:
        mastertoken: "{{ output.stdout }}"

    - name: debug master token
      debug:
        msg: "{{ output }}"

    - name: debug master token
      debug:
        msg: "{{ output.stdout }}"

    - name: Get kube config file
      fetch:
        # src: /home/mierdin/.kube/config
        # dest: ./kubeconfig
        # mode: 0774
        src: /home/mierdin/.kube/config
        dest: tmp/kubeconfig
        flat: yes

- name: Set up K8S Workers
  hosts: kubernetes_workers
  become: yes
  tasks:

  #TODO - temporay measure - bridge creation should take place in syringe
  # - name: Set up bridges
  #   shell: "{{ item }}"
  #   with_items:
  #   - brctl addbr weave-brmgmt
  #   - brctl addbr weave-br12
  #   - brctl addbr weave-br23
  #   - brctl addbr weave-br31

  - name: docker info for getting cgroup driver
    shell: docker info 2>/dev/null |grep "Cgroup Driver" |awk '{print $3}'
    register: docker_info

  - name: get cgroup driver
    set_fact:
      cgroup_driver: "{{ docker_info.stdout }}"

  - name: set cgroup driver to cgroupfs
    lineinfile:
      path: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
      regexp: '^Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=.*"'
      line: 'Environment="KUBELET_CGROUP_ARGS=--cgroup-driver={{ cgroup_driver }}"'

  - name: Upload kubeconfig
    copy:
      src: tmp/kubeconfig
      dest: /etc/kubernetes/node-kubeconfig.yaml
      force: no

  - name: Clone multus repo to instance
    git:
      repo: 'https://github.com/intel/multus-cni'
      dest: ~/multus-cni/
      force: yes
    ignore_errors: yes

  - name: Compile multus
    shell: cd ~/multus-cni && ./build

  - name: Install multus
    shell: cp ~/multus-cni/bin/multus /opt/cni/bin

  - name: Create CNI directory
    file:
      path: /etc/cni/net.d/
      state: directory

  # Important that we specify a priority of 1 as most other plugins will try to generate
  # their own conf files at a priority of 10
  - name: Install multus CNI config
    template:
      src: templates/multus-cni.conf
      dest:  /etc/cni/net.d/1-multus-cni.conf

  - name: enable kubelet service
    systemd:
      name: kubelet
      state: restarted
      daemon_reload: yes
      enabled: True

  - name: join k8s cluster
    shell: "kubeadm join --token {{ hostvars['tf-controller0.labs.networkreliability.engineering'].mastertoken }} --discovery-token-unsafe-skip-ca-verification tf-controller0:6443"
    ignore_errors: yes

- name: Antidoty things
  hosts: kubernetes_hosts
  vars:
    ansible_ssh_private_key_file: ~/.ssh/google_compute_engine
  gather_facts: no
  tasks:

    - stat: path=~/.ssh/id_rsa.pub
      register: ssh_key_exists

    - debug:
        msg: "{{ ssh_key_exists }}"

    # TODO(mierdin): Need to provision an SSH key per lab, but this works for now. 
    # Will just have to copy this key to the machine you want to SSH to the vmx on for now
    - name: Copy public SSH key to vMX
      shell: ssh-keygen -b 2048 -t rsa -f ~/.ssh/id_rsa -q -N ""
      when: ssh_key_exists.stat.exists == False

    - name: Create directory for vMX files
      file:
        path: ~/myfiles
        state: directory
        mode: 0755

    - stat: path=~/myfiles/junos-vmx-x86-64-18.1R1.9.qcow2
      register: image_exists

    - debug:
        msg: "{{ image_exists }}"

    - name: Download vMX Image
      shell: curl -o ~/myfiles/junos-vmx-x86-64-18.1R1.9.qcow2 https://storage.googleapis.com/antidote-files/junos-vmx-x86-64-18.1R1.9.qcow2
      when: image_exists.stat.exists == False

    - stat: path=~/myfiles/id_rsa.pub
      register: ssh_key_copied

    - debug:
        msg: "{{ ssh_key_copied }}"

    - name: Copy public SSH key to vMX
      shell: cp ~/.ssh/id_rsa.pub ~/myfiles/id_rsa.pub
      when: ssh_key_copied.stat.exists == False

    - name: Download vMX eval
      shell: curl -o ~/myfiles/license-eval.txt https://www.juniper.net/us/en/dm/free-vmx-trial/E421992502.txt

    - name: Download and load csrx image
      become: yes  
      shell: curl -o csrx-18.1R1.9.img.bz2 https://storage.googleapis.com/antidote-files/csrx-18.1R1.9.img.bz2 && docker image load -i csrx-18.1R1.9.img.bz2

    - name: Remove antidote repo
      shell: rm -rf ~/antidote

    - name: Clone antidote repo to instance
      git:
        repo: 'https://github.com/nre-learning/antidote'
        dest: ~/antidote/
        force: yes

    # TODO(mierdin): Again, we'll have to create an SSH key for each lab instance
    - name: Copy SSH keys to lessons dir
      shell: mkdir -p ~/antidote/lessons/ssh && cp ~/.ssh/id_rsa ~/antidote/lessons/ssh/id_rsa && cp -r ~/.ssh/id_rsa.pub ~/antidote/lessons/ssh/id_rsa.pub

    - name: TIME TO DO THIS?
      debug:
        msg: |

          try the blow,may be useful - maybe only for flannel though. Need to test for weave without this first
          edit /etc/kubernetes/manifests/kube-controller-manager.yaml
          at command ,add
          --allocate-node-cidrs=true
          --cluster-cidr=10.244.0.0/16
          then,reload all kubelets


- name: Set up weave networking
  hosts: kubernetes_master
  tasks:

  - name: install weave via vendored manifest
    shell: kubectl apply -f ~/antidote/infra/weaveinstall.yml


# I don't **think** I need to restart the kubelet now, this just takes care of deploying the binaries.

